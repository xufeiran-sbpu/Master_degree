---
title: "Practice work 2” - Xu Feiran - 21.M09"
author: "Xu Feiran"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    highlight: tango
    theme: cerulean
    toc: yes
    toc_depth: 3
link-citations: yes
biblio-style: apalike
---

```{r}
szstock<-read.csv("./Datasets-master/monthly-mean-temp.csv")
MyData <- szstock$Temperature
tsdata <- ts(MyData, start =c(1920,1),frequency = 12)
plot.ts(tsdata)
```
Mean monthly air temperature (Deg. F) Nottingham Castle 1920-1939

Source: Time Series Data Library (citing: O.D. Anderson (1976))

```{r}
MyData.acf <- acf(MyData)
MyData.acf
```
Autocorrelations of series ‘MyData’, by lag

     0      1      2      3      4      5      6      7      8      9     10     11     12     13     14     15     16     17     18     19     20     21     22     23 
 1.000  0.808  0.452 -0.018 -0.464 -0.770 -0.876 -0.756 -0.445 -0.010  0.429  0.765  0.884  0.770  0.435 -0.011 -0.441 -0.734 -0.844 -0.726 -0.431 -0.015  0.420  0.732 

```{r}
MyData.pacf<-pacf(MyData)
MyData.pacf
```
Here are the ACF and PACF of this time series. We can see straight away that this time series is unstable.


Using diff function in R library forecast to create the difference series. 
```{r}
diff1 <- diff(tsdata,lag = 1)
acf(diff1)
```
```{r}
diff2 <- diff(tsdata,lag = 2)
acf(diff2)
```
ADF test. 
H0: There exist the unit root.(not stationary series) 
H1: There doesn't exist the unit root.(stationary series)
```{r}
library(tseries)
adf.test(tsdata)
adf.test(diff1[-1])
adf.test(diff2[-2])
```
This result show that we shoud reject the null hypothesis, which means the process is stationary.

We can use ndiffs function to find the order.
```{r}
library(forecast)
ndiffs(tsdata)
```
The result show us the order of stationary is 0 . As stated above, this time series is inherently stable.


From the ACF and PACF figures, i think we chose p = 3 and q= 2 will be good. And as i told before, d=0.
So i construct ARIMA(3,0,2) model.
```{r}
fit_model <- Arima(diff1,order = c(3,0,2))
fit_model
```

```{r}
checkresiduals(fit_model)
```
There hasn't autocorrelation for low order members. 
```{r}
plot(decompose(tsdata,type = "additive"))
```
I tried to use function decompose to find the seasonal component, and we can easily saw there has seasonal component in my time series, and the seasonal cycle is equal to 12. And here is the Seasonal Difference.
```{r}
seas_diff <- diff(tsdata, lag = 12)
plot(seas_diff)
```
```{r}
seas_diff_fit <- auto.arima(tsdata,seasonal = T)
seas_diff_fit
```
```{r}
checkresiduals(seas_diff_fit)
```
In this case, the p-value = 0.3935 is greater than 0.05, so we have to reject the H0, that means the residuals of this model is white-noise. 
```{r}
seas_diff_fit.residual <- residuals(seas_diff_fit)
shapiro.test(seas_diff_fit.residual)
```
The p-value=0.2543 is greater than 0.05, that mean the H0 can not be rejected, so the residuals is normality.
```{r}
  force_best <- forecast(Arima(tsdata,order=c(1,0,2),seasonal=c(1,1,2),lambda=0),h=216)
  force_intuition <- forecast(Arima(tsdata,order=c(2,0,2),seasonal=c(2,2,2),lambda=0),h=216)
  autoplot(force_best)
  autoplot(force_intuition)
```

```{r}
summary(force_best)
summary(force_intuition)
```
Based on this result, we can see that both models predict well, but the first model has a smaller AIC.
```{r}
car_sale<-read.csv("./Datasets-master/monthly-car-sales.csv")
MyData1 <- car_sale$Sales
tsdata1 <- ts(MyData1, start =c(1960,1),frequency = 12)
plot.ts(tsdata1)
```
Monthly car sales receive seasonal influences and are seasonal in nature and increase from year to year.
```{r}
acf(MyData1)
```

```{r}
pacf(MyData1)
```

```{r}
plot(decompose(tsdata1,type = "additive"))
```
I used the decompose function to find the seasonal component , and from the ACF plot, we can see at lag=12, the ACF is very high, so the length of seasonality is 12.
From the image, the fluctuation of the sequence grows with the overall level.Seasonally, cars are sold a little more in the summer.


```{r}
  fit_model1 <- Arima(tsdata1,order=c(1,0,2),seasonal=c(1,1,2),lambda=0)
  summary(fit_model1)
```
```{r}
fit_model1.residuals <- residuals(fit_model)
shapiro.test(fit_model1.residuals)

```
I used the Shapiro test to check the disrtibution of errors of the ARIMA(1,0,2)(1,1,2)[12] model, the p-calue=0.3176 > 0.05 , so we couldn't reject the H0, that means the error has normal distribution. 

```{r}
  fit_model <- Arima(tsdata1,order=c(1,1,2),lambda=0)
  summary(fit_model)
```
```{r}
fit_autochose <- auto.arima(tsdata1,seasonal = TRUE, ic = "aic",D=1)
summary(fit_autochose)
```
Based on the RMSE and MAE, our best model in this case is ARIMA(1,0,1)(0,1,1)[12]. 
```{r}
checkresiduals(fit_autochose)
```

```{r}
fit_autochose.residuals <- residuals(fit_autochose)
fit_autochose.fitted <- tsdata1 - fit_autochose.residuals
ts.plot(tsdata1)
points(fit_autochose.fitted, type = "l", col = 2, lty = 2)
```
Use the best model to forecast 2 years ahead 

```{r}
autoplot(forecast(fit_autochose,h=24))
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

